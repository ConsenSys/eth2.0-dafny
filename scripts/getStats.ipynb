{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit72dfc5626a924633aeb96a8e35da9f04",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*\n",
    " * Copyright 2020 ConsenSys AG.\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\"); you may \n",
    " * not use this file except in compliance with the License. You may obtain \n",
    " * a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software dis-\n",
    " * tributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT \n",
    " * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n",
    " * License for the specific language governing permissions and limitations \n",
    " * under the License.\n",
    " */"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "This notebook contains some basic processing to automate the collection of statistics relating to the Dafny files.\n",
    "By creating functions to perform analysis of Dafny files, additional results can easily be added to the pandas dataframe.\n",
    "The use of a pandas dataframe provides many options for visualisation and the data can easily by stored in a csv.\n",
    "The data can also easily be supplemented with timestamps to faciliate time series analysis.\n",
    "\n",
    "This file is a working file and will be converted to a python script in due course."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Reformat function documentation to standard style used within this repo"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File processing functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find *.dfy files, within a given local repo path\n",
    "# this function will search all subfolders of dirName\n",
    "# a sorted list of files is returned\n",
    "def getListOfDafnyFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # if entry is a directory then append the list of files in this directory to allFiles\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfDafnyFiles(fullPath)\n",
    "        # else append file only if it is a Dafny file\n",
    "        else:\n",
    "            if entry.endswith(\".dfy\"):\n",
    "                allFiles.append(fullPath)\n",
    "    return sorted(allFiles)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find folders within the repo that have *.dfy files\n",
    "# a sorted list of folders is returned (i.e. full path of each folder)\n",
    "def getListOfDafnyFolders(dafnyFiles):\n",
    "    listOfDirectories = list()\n",
    "    for file in dafnyFiles:\n",
    "        listOfDirectories.append(os.path.dirname(file))\n",
    "    return sorted(list(set(listOfDirectories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get folder for an individual dafny file\n",
    "# i.e. for the full path of a dafny file, the filename and repo path are striped\n",
    "def getFolder(repo, dafny_file):\n",
    "    repo_path, folder = os.path.dirname(dafny_file).split(repo,1)\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test file processing functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test getListOfDafnyFiles: \n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/merkle/Merkleise.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/BitListSeDes.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/BoolSeDes.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/BytesAndBits.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/IntSeDes.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/Serialise.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/DafTests.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/Eth2Types.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/Helpers.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/MathHelpers.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/NativeTypes.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils/SeqHelpers.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/merkle/Merkleise.test.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/ssz/BitListSeDes.tests.dfy\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/utils/MathHelpers.tests.dfy\nLength of returned list:  15\nTest getListOfDafnyFolders: \n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/merkle\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/merkle\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/ssz\n/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/utils\nLength of returned list:  6\nTest getFolder for each file in files: \nsrc/dafny/merkle\nsrc/dafny/ssz\nsrc/dafny/ssz\nsrc/dafny/ssz\nsrc/dafny/ssz\nsrc/dafny/ssz\nsrc/dafny/utils\nsrc/dafny/utils\nsrc/dafny/utils\nsrc/dafny/utils\nsrc/dafny/utils\nsrc/dafny/utils\ntest/dafny/merkle\ntest/dafny/ssz\ntest/dafny/utils\n"
    }
   ],
   "source": [
    "# test the getListOfDafnyFiles, getListOfDafnyFolders and getFolder functions\n",
    "# local repo path needs to be set prior to running the tests\n",
    "repo_directory = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/\"\n",
    "\n",
    "print(\"Test getListOfDafnyFiles: \")\n",
    "files = getListOfDafnyFiles(repo_directory)\n",
    "for i in files:\n",
    "    print(i)\n",
    "print(\"Length of returned list: \", len(files))\n",
    "\n",
    "print(\"Test getListOfDafnyFolders: \")\n",
    "directories = getListOfDafnyFolders(files)\n",
    "for i in directories:\n",
    "    print(i)\n",
    "print(\"Length of returned list: \", len(directories))\n",
    "\n",
    "print(\"Test getFolder for each file in files: \")\n",
    "for file in files:\n",
    "    print(getFolder(repo_directory, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to collect statistics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of \"lemmas\" in a given dafny file\n",
    "# this function uses a subprocess call\n",
    "# an alternative method would be to read and search the file directly\n",
    "def getLemmas(dafny_file):\n",
    "    cmd = \"cat \" + dafny_file +\"| grep lemma | wc -l\"\n",
    "    result = subprocess.run(['/bin/bash', '-i', '-c', cmd], stdout=subprocess.PIPE)\n",
    "    return result.stdout.strip().decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of \"function methods\" in a given dafny file\n",
    "# this function uses a subprocess call\n",
    "# an alternative method would be to read and search the file directly\n",
    "def getFunctions(dafny_file):\n",
    "    cmd = \"cat \" + dafny_file +\"| grep function | grep method | wc -l\"\n",
    "    result = subprocess.run(['/bin/bash', '-i', '-c', cmd], stdout=subprocess.PIPE)\n",
    "    return result.stdout.strip().decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of ghost (= function and lemmas) processes\n",
    "# ignores function methods\n",
    "# to be referred to as \"Theorems\" in the data display\n",
    "def getGhost(dafny_file):\n",
    "    tmp_file = open(dafny_file, \"r\")\n",
    "    count = 0\n",
    "    for line in tmp_file.readlines():\n",
    "        if line.strip().startswith((\"function\", \"lemma\")):\n",
    "            if not line.strip().startswith(\"function method\"):\n",
    "                count += 1\n",
    "                #print(line)\n",
    "    tmp_file.close()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of non-ghost ()= function methods and methods and predicates) processes\n",
    "# to be referred to as \"Implementations\" in the data display\n",
    "def getNonGhost(dafny_file):\n",
    "    tmp_file = open(dafny_file, \"r\")\n",
    "    count = 0\n",
    "    for line in tmp_file.readlines():\n",
    "        if line.strip().startswith((\"function method\", \"method\", \"predicate\")):\n",
    "            count += 1\n",
    "            #print(line)\n",
    "\n",
    "    tmp_file.close()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of lines of code\n",
    "# the count occurs after the dafny file is printed used the compiler\n",
    "# the count also occurs after this output has been cleaned\n",
    "def getLoC(dafny_file):\n",
    "    show_ghost = True\n",
    "    executable = \"dafny\"\n",
    "    args  = [] \n",
    "    args += ['/rprint:-']\n",
    "    args += [\"/noAutoReq\"]\n",
    "    args += [\"/noVerify\"]\n",
    "    args += [\"/nologo\"]\n",
    "    args += [\"/env:0\"]\n",
    "    if show_ghost:\n",
    "        args += [\"/printMode:NoIncludes\"]\n",
    "    else:\n",
    "        args += [\"/printMode:NoGhost\"]\n",
    "    args += [dafny_file]\n",
    "    cmd = ' '.join([executable] + args)\n",
    "    result = subprocess.run(['/bin/bash', '-i', '-c', cmd], stdout=subprocess.PIPE)\n",
    "    output = result.stdout.decode('ascii')\n",
    "    #print(type(result.stdout.decode('ascii')))\n",
    "    #print(result.stdout.decode('ascii'))\n",
    "\n",
    "    #remove this section once code has be tested OR comment out\n",
    "    #tmp_file = open(\"tmp.txt\", \"w\")\n",
    "    #tmp_file.write(result.stdout.decode('ascii'))\n",
    "    #tmp_file.close()\n",
    "    ######---------------------\n",
    "\n",
    "    count = 0\n",
    "    for line in output.splitlines():\n",
    "        # clean output i.e. remove comment at start and verifier status\n",
    "        if line.startswith((\"Dafny program verifier finished\", \"//\")):\n",
    "            #print(i)\n",
    "            pass\n",
    "        else:\n",
    "            if line.strip():\n",
    "                count += 1\n",
    "                #print(line)\n",
    "    #print(\"#LoC: \", count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of lines included in the license comment\n",
    "# assumes license comment is at the start of the file and is of format /* ... */\n",
    "# assumes that it has been confirmed that the file has a license comment\n",
    "def getLicenseLineCount(dafny_file):\n",
    "    tmp_file = open(dafny_file, \"r\")\n",
    "    count = 0\n",
    "    flag = 0\n",
    "    for line in tmp_file.readlines():\n",
    "        tmp_line = line.strip()\n",
    "        cleaned = ' '.join(i for i in tmp_line.split() if i not in [\"//\", \"/*\", \"/**\", \"*\", \"*/\"])\n",
    "        if (not flag) and (tmp_line.startswith(\"/*\")):\n",
    "            if cleaned:\n",
    "                count += 1\n",
    "            flag = 1\n",
    "        elif flag:\n",
    "            if cleaned:\n",
    "                count += 1\n",
    "            if tmp_line.startswith(\"*/\"):\n",
    "                tmp_file.close()\n",
    "                return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of lines of documentation\n",
    "# don't include license comment or empty comment lines\n",
    "def getDocumentation(dafny_file):\n",
    "    tmp_file = open(dafny_file, \"r\")\n",
    "    count = 0\n",
    "    license_flag = 0\n",
    "    for line in tmp_file.readlines():\n",
    "        tmp_line = line.strip()\n",
    "        if tmp_line.startswith((\"//\", \"/*\", \"/**\", \"*\", \"*/\")):\n",
    "            cleaned = ' '.join(i for i in tmp_line.split() if i not in [\"//\", \"/*\", \"/**\", \"*\", \"*/\"])\n",
    "            if cleaned:\n",
    "                #print(cleaned)\n",
    "                count += 1\n",
    "                #print(line)\n",
    "        if tmp_line.startswith(\"* Copyright 2020 ConsenSys AG.\"):\n",
    "            license_flag = 1\n",
    "\n",
    "    tmp_file.close()\n",
    "    if license_flag:\n",
    "        count -= getLicenseLineCount(dafny_file)\n",
    "        #print(getLicenseLineCount(dafny_file))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of theorems (getGhost) and implementations (getNonGhost) proved\n",
    "# i.e. check that the number of errors when verified is zero\n",
    "# TODO: include arguments for getGhost and getNonGhost to reduce duplicate processing\n",
    "def getProved(dafny_file):\n",
    "    cmd = \"dafny /dafnyVerify:1 /compile:0 \" + dafny_file\n",
    "    result = subprocess.run(['/bin/bash', '-i', '-c', cmd], stdout=subprocess.PIPE)\n",
    "    output = result.stdout.decode('ascii')\n",
    "    for line in output.splitlines():\n",
    "        if line.startswith(\"Dafny program verifier finished with \"):\n",
    "            # check no errors\n",
    "            #print(line, re.findall(r'\\d+', line)[1], type(re.findall(r'\\d+', line)[1]))\n",
    "            if not int(re.findall(r'\\d+', line)[1]):\n",
    "                return (getGhost(dafny_file) + getNonGhost(dafny_file))\n",
    "        else:\n",
    "            pass\n",
    "    # if the verifier doesn't finish, return -1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test statistics functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Proved (verified from compile) ...\n8\n"
    }
   ],
   "source": [
    "# test file options:\n",
    "test_file = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/BytesAndBits.dfy\"\n",
    "#test_file = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/merkle/Merkleise.test.dfy\"\n",
    "#test_file = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/dafny/ssz/BitListSeDes.tests.dfy\"\n",
    "#test_file = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz/BitListSeDes.dfy\"\n",
    "#test_file = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/merkle/Merkleise.dfy\"\n",
    "\n",
    "#print(\"Lemmas ...\")\n",
    "#print(getLemmas(test_file))\n",
    "\n",
    "#print(\"Function methods ...\")\n",
    "#print(getFunctions(test_file))\n",
    "\n",
    "#print(\"LoC ...\")\n",
    "#print(getLoC(test_file))\n",
    "\n",
    "#print(\"Documentation ...\")\n",
    "#print(getDocumentation(test_file))\n",
    "\n",
    "print(\"Proved (verified from compile) ...\")\n",
    "print(getProved(test_file))\n",
    "\n",
    "#print(\"Ghost ...\")\n",
    "#rint(getGhost(test_file))\n",
    "\n",
    "#print(\"NonGhost ...\")\n",
    "#print(getNonGhost(test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate results into a pandas dataframe\n",
    "\n",
    "One row per Dafny file."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"['#Doc/#LoC'] not in index\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b690d1feb711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# convert numeric columns to int64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnumCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'#LoC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Theorems'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Implementations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Documentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#Doc/#LoC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Proved\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#display a sample of rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['#Doc/#LoC'] not in index\""
     ]
    }
   ],
   "source": [
    "# create a pandas dataframe to store stats relating to the dafny files\n",
    "column_list = ['Files', 'Folder', '#LoC', 'Theorems', 'Implementations', \"Documentation\", \"#Doc/#LoC (%)\", \"Proved\"]\n",
    "repo_directory = \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/\"\n",
    "files = getListOfDafnyFiles(repo_directory)\n",
    "\n",
    "df = pd.DataFrame(columns=column_list)\n",
    "\n",
    "# collect data for each dafny file\n",
    "for file in files:\n",
    "    loc = getLoC(file)\n",
    "    ghost = getGhost(file)\n",
    "    nonghost = getNonGhost(file)\n",
    "    doc = getDocumentation(file)\n",
    "    proved = getProved(file)\n",
    "    df2 = pd.DataFrame([[os.path.basename(file), \n",
    "                        getFolder(repo_directory, file), \n",
    "                        loc ,\n",
    "                        ghost, \n",
    "                        nonghost,\n",
    "                        doc,\n",
    "                        round(doc/loc * 100),\n",
    "                        proved]], \n",
    "                        columns=column_list)\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "\n",
    "# create and append totals for numeric columns\n",
    "totals = pd.DataFrame([[\"\", \n",
    "                        \"TOTAL\", \n",
    "                        df['#LoC'].sum(),\n",
    "                        df['Theorems'].sum(), \n",
    "                        df['Implementations'].sum(),\n",
    "                        df['Documentation'].sum(),\n",
    "                        round(df['Documentation'].sum()/df['#LoC'].sum() * 100),\n",
    "                        df['Proved'].sum()]], \n",
    "                        columns=column_list)\n",
    "df = df.append(totals, ignore_index=True)\n",
    "\n",
    "# convert numeric columns to int64\n",
    "numCols = ['#LoC', 'Theorems', 'Implementations', \"Documentation\", \"#Doc/#LoC\", \"Proved\"]\n",
    "df[numCols] = df[numCols].astype(\"int64\")\n",
    "\n",
    "#display a sample of rows\n",
    "df.head(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative format\n",
    "\n",
    "May be useful for github"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "|    | Files                  | Folder            |   #LoC |   Theorems |   Implementations |   Documentation |   #Doc/#LoC (%) |   Proved |\n|----|------------------------|-------------------|--------|------------|-------------------|-----------------|-----------------|----------|\n|  0 | Merkleise.dfy          | src/dafny/merkle  |     97 |          7 |                 4 |              60 |              62 |       11 |\n|  1 | BitListSeDes.dfy       | src/dafny/ssz     |    176 |          7 |                 3 |              58 |              33 |       10 |\n|  2 | BoolSeDes.dfy          | src/dafny/ssz     |     18 |          0 |                 2 |               3 |              17 |        2 |\n|  3 | BytesAndBits.dfy       | src/dafny/ssz     |     60 |          3 |                 5 |              26 |              43 |        8 |\n|  4 | IntSeDes.dfy           | src/dafny/ssz     |     20 |          1 |                 2 |               4 |              20 |        3 |\n|  5 | Serialise.dfy          | src/dafny/ssz     |     59 |          3 |                 3 |              21 |              36 |        6 |\n|  6 | DafTests.dfy           | src/dafny/utils   |     47 |          0 |                 4 |              25 |              53 |        4 |\n|  7 | Eth2Types.dfy          | src/dafny/utils   |     53 |          3 |                 1 |              35 |              66 |        4 |\n|  8 | Helpers.dfy            | src/dafny/utils   |    195 |         10 |                 2 |              50 |              26 |       12 |\n|  9 | MathHelpers.dfy        | src/dafny/utils   |    133 |          6 |                 2 |              19 |              14 |        8 |\n| 10 | NativeTypes.dfy        | src/dafny/utils   |     28 |          0 |                 0 |              13 |              46 |        0 |\n| 11 | SeqHelpers.dfy         | src/dafny/utils   |     38 |          6 |                 0 |              12 |              32 |        6 |\n| 12 | Merkleise.test.dfy     | test/dafny/merkle |     19 |          0 |                 1 |              10 |              53 |        1 |\n| 13 | BitListSeDes.tests.dfy | test/dafny/ssz    |     14 |          0 |                 1 |               7 |              50 |        1 |\n| 14 | MathHelpers.tests.dfy  | test/dafny/utils  |     13 |          0 |                 1 |              17 |             131 |        1 |\n| 15 |                        | TOTAL             |    970 |         46 |                31 |             360 |              37 |       77 |\n"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(df, headers='keys', tablefmt='github'))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data\n",
    "\n",
    "One row per folder."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              Folder #Files #LoC Theorems Implementations Documentation  \\\n0  /src/dafny/merkle      1   97        7               4            60   \n1     /src/dafny/ssz      5  333       14              15           112   \n2   /src/dafny/utils      6  494       25               9           154   \n3  /test/dafny/utils      3   46        0               3            34   \n\n  #Doc/#LoC (%) Proved  \n0            62     11  \n1            34     29  \n2            31     34  \n3            74      3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Folder</th>\n      <th>#Files</th>\n      <th>#LoC</th>\n      <th>Theorems</th>\n      <th>Implementations</th>\n      <th>Documentation</th>\n      <th>#Doc/#LoC (%)</th>\n      <th>Proved</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/src/dafny/merkle</td>\n      <td>1</td>\n      <td>97</td>\n      <td>7</td>\n      <td>4</td>\n      <td>60</td>\n      <td>62</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/src/dafny/ssz</td>\n      <td>5</td>\n      <td>333</td>\n      <td>14</td>\n      <td>15</td>\n      <td>112</td>\n      <td>34</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/src/dafny/utils</td>\n      <td>6</td>\n      <td>494</td>\n      <td>25</td>\n      <td>9</td>\n      <td>154</td>\n      <td>31</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/test/dafny/utils</td>\n      <td>3</td>\n      <td>46</td>\n      <td>0</td>\n      <td>3</td>\n      <td>34</td>\n      <td>74</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# create a pandas dataframe to store stats relating to the dafny files\n",
    "# stats grouped by folder\n",
    "folders = [\"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/merkle\",\n",
    "            \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/ssz\",\n",
    "            \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/src/dafny/utils\",\n",
    "            \"/Users/joannefuller/Documents/vscode/eth2.0-dafny/test/\"]\n",
    "\n",
    "column_list = ['Folder', '#Files', '#LoC', 'Theorems', 'Implementations', \"Documentation\", \"#Doc/#LoC (%)\", \"Proved\"]\n",
    "df_grouped = pd.DataFrame(columns=column_list)\n",
    "\n",
    "for folder in folders:\n",
    "    files = getListOfDafnyFiles(folder)\n",
    "    \n",
    "    nFiles = 0\n",
    "    nLoc = 0\n",
    "    nGhost = 0\n",
    "    nNonGhost = 0\n",
    "    nDoc = 0\n",
    "    nProved = 0\n",
    "    for file in files:\n",
    "        nFiles += 1\n",
    "        nLoc += getLoC(file)\n",
    "        nGhost += getGhost(file)\n",
    "        nNonGhost += getNonGhost(file)\n",
    "        nDoc += getDocumentation(file)\n",
    "        nProved += getProved(file)\n",
    "    \n",
    "\n",
    "    df2 = pd.DataFrame([[getFolder(\"/Users/joannefuller/Documents/vscode/eth2.0-dafny\", file), \n",
    "                        nFiles, \n",
    "                        nLoc ,\n",
    "                        nGhost, \n",
    "                        nNonGhost,\n",
    "                        nDoc,\n",
    "                        round(nDoc/nLoc * 100),\n",
    "                        nProved]], \n",
    "                        columns=column_list)\n",
    "    df_grouped = df_grouped.append(df2, ignore_index=True)\n",
    "\n",
    "#display a sample of rows\n",
    "df_grouped.head(len(df_grouped))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print dataframe to .csv, .tex and .pdf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'data/data20200415.pdf'"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# create filenames that include the current data string\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "rawfile = 'data' + timestr + '.csv'\n",
    "grouped_rawfile = 'dataGrouped' + timestr + '.csv'\n",
    "filename = 'data' + timestr + '.tex'\n",
    "pdffile = 'data' + timestr + '.pdf'\n",
    "\n",
    "# check if data directory already exists and create if necessary\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "#print to csv file without an index\n",
    "df.to_csv(\"data/\" + rawfile, index = False)\n",
    "df_grouped.to_csv(\"data/\" + grouped_rawfile, index = False)\n",
    "\n",
    "#print to pdf via latex\n",
    "template = r'''\\documentclass[a4paper, 12pt]{{article}}\n",
    "\\usepackage[landscape]{{geometry}}\n",
    "\\usepackage{{booktabs}}\n",
    "\\begin{{document}}\n",
    "\\section*{{https://github.com/PegaSysEng/eth2.0-dafny}}\n",
    "\\subsection*{{Data collected: {}}}\n",
    "\\scriptsize\n",
    "{}\n",
    "\\vspace{{2em}}\n",
    "{}\n",
    "\\end{{document}}\n",
    "'''\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(template.format(time.strftime(\"%Y-%m-%d\"), df.to_latex(index=False), df_grouped.to_latex(index=False)))\n",
    "\n",
    "subprocess.call(['pdflatex', filename])\n",
    "\n",
    "# remove surplus files and move .csv, .tex and .pdf files to the data folder\n",
    "os.remove('data' + timestr + '.log')\n",
    "os.remove('data' + timestr + '.aux')\n",
    "\n",
    "shutil.move(filename, \"data/\" + filename)\n",
    "shutil.move(pdffile, \"data/\" + pdffile)"
   ]
  }
 ]
}